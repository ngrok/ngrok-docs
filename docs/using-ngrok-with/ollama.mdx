import ConfigExample from "/src/components/ConfigExample.tsx";

# Using ngrok with Ollama

Ollama is a locally deployed AI model runner, designed to allow users to download and execute large language models (LLMs) locally on your machine. A perfect pairing for ngrok. By combining Ollama with ngrok, you can give your local Ollama an endpoint on the internet, enabling remote access and integration with other applications.

## What you'll need

- The [ngrok agent](https://ngrok.com/download) installed on your system
- An [Ollama](https://ollama.com/) instance running on the same system
- An [ngrok account](https://ngrok.com/signup)

## 1. Connect the agent to your ngrok account

Sign up for an account at [ngrok.com](https://ngrok.com/) and run:

```bash
ngrok config add-authtoken $YOUR_NGROK_AUTH_TOKEN
```

## 2. Install and run Ollama on your machine

[Download Ollama](https://ollama.com/download) by following the instructions on the Ollama website and [search for a model](https://ollama.com/search) you'd like to use.

Pull the model you've chosen to your instance:

```bash
ollama pull gemma3
```

Start the Ollama server:

```bash
ollama serve
```

By default, Ollama will start on `http://localhost:11434`.

## 3. Create an endpoint for your Ollama server

In a new terminal window, start an ngrok tunnel to your local Ollama port:

```bash
ngrok http 11434 --host-header=localhost
```

ngrok will generate a public forwarding URL like `https://abcd1234.ngrok.app`.
This URL now provides public access to your local Ollama instance.

## 4. Use your Ollama instance from anywhere

You can now send requests to your Ollama server from anywhere using the ngrok URL.
For example, run the `curl` command below, replacing `abcd1234.ngrok.app` with your domain name and `gemma3` with a model you've pulled, to prompt your LLM.

```bash
curl https://abcd1234.ngrok.app/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "gemma3",
        "messages": [
          {
            "role": "system",
            "content": "You are a helpful assistant."
          },
          {
            "role": "user",
            "content": "Hello!"
          }
        ]
      }'
```

Last thing, you now have a public endpoint for your Ollama instance, which means anyone on the internet could find it and use it.

## 5. Protect your Ollama instance with basic auth

You may not want everyone to be able to access your LLM. ngrok can quickly add authentication to your LLM without any changes.
Explore [Traffic Policy](/docs/traffic-policy) to understand all the ways ngrok can protect your endpoint.

Create a new `traffic-policy.yml` file and paste in the policy below, which uses the [`basic-auth` Traffic Policy action](/traffic-policy/actions/basic-auth/) to only allow visitors with the credentials `user:password1` or `admin:password2` to access your app.

```yaml
on_http_request:
  - actions:
      - type: basic-auth
        config:
          realm: sample-realm
          credentials:
            - user:password1
            - admin:password2
          enforce: true
```

Start the agent again with the `--traffic-policy-file` flag:

```bash
ngrok http 11434 --traffic-policy-file traffic-policy.yml --host-header=localhost
```

You can test your traffic policy by sending the same LLM prompt to Ollama's API with the `Authorization: Basic` header and a [base64 encoded](https://en.wikipedia.org/wiki/Base64) version of `user:password1`.

```shell
curl https://abcd1234.ngrok.app/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H 'Authorization: Basic dXNlcjpwYXNzd29yZDE='
  -d '{
        "model": "gemma3",
        "messages": [
          {
            "role": "system",
            "content": "You are a helpful assistant."
          },
          {
            "role": "user",
            "content": "Hello!"
          }
        ]
      }'
```

If you send the same request without the `Authorization` header, you should receive a `401 Unauthorized` response.

Your personal LLM is now locked down to only accept authenticated users.
