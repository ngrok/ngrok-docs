---
title: Ollama
sidebarTitle: Ollama
description: Route AI requests to local Ollama models through the ngrok AI Gateway
---

[Ollama](https://ollama.ai) is a popular tool for running open-source large language models locally. This guide shows you how to connect Ollama to the ngrok AI Gateway, enabling you to use local models through the OpenAI SDK with automatic failover to cloud providers.

## Prerequisites

- [ngrok account](https://dashboard.ngrok.com/signup) with AI Gateway access
- [Ollama](https://ollama.ai/download) installed locally
- [ngrok agent](https://download.ngrok.com) installed

## Overview

Since Ollama runs locally on HTTP, you'll expose it through an ngrok internal endpoint, then configure the AI Gateway to route requests to it.

```mermaid
graph LR
    A[Client] --> B[AI Gateway]
    B --> C[ngrok Internal Endpoint]
    C --> D[Ollama localhost:11434]
```

## Step 1: Start Ollama

Start the Ollama server:

```bash
ollama serve
```

Pull a model if you haven't already:

```bash
ollama pull llama3.2
```

Verify Ollama is running:

```bash
curl http://localhost:11434/api/tags
```

## Step 2: Expose Ollama with ngrok

Use the [ngrok agent](/agent/) to create an internal endpoint:

```bash
ngrok http 11434 --url https://ollama.internal
```

<Note>
Internal endpoints (`.internal` domains) are private to your ngrok account. They're not accessible from the public internet.
</Note>

## Step 3: Create your AI Gateway endpoint

Create a cloud endpoint for your AI Gateway. In your ngrok dashboard or via the API:

```bash
ngrok api endpoints create \
  --url https://your-ai-subdomain.ngrok.app \
  --traffic-policy-file policy.yaml
```

## Step 4: Configure the traffic policy

Create a `policy.yaml` file with your AI Gateway configuration:

```yaml title="policy.yaml" highlight={6-12}
on_http_request:
  - type: ai-gateway
    config:
      providers:
        - id: "ollama"
          base_url: "https://ollama.internal"
          models:
            - id: "llama3.2"
            - id: "llama3.2:1b"
            - id: "mistral"
            - id: "codellama"
```

<Tip>
Ollama doesn't require API keys, so you can omit the `api_keys` field entirely.
</Tip>

## Step 5: Use with OpenAI SDK

Point any OpenAI-compatible SDK at your AI Gateway endpoint:

<CodeGroup>
```python Python highlight={4,9}
from openai import OpenAI

client = OpenAI(
    base_url="https://your-ai-subdomain.ngrok.app/v1",
    api_key="unused"  # Ollama doesn't need a key
)

response = client.chat.completions.create(
    model="ollama:llama3.2",  # Prefix with provider ID
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

```typescript TypeScript highlight={4,9}
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "https://your-ai-subdomain.ngrok.app/v1",
  apiKey: "unused"  // Ollama doesn't need a key
});

const response = await client.chat.completions.create({
  model: "ollama:llama3.2",  // Prefix with provider ID
  messages: [{ role: "user", content: "Hello!" }]
});

console.log(response.choices[0].message.content);
```

```bash cURL highlight={1,4}
curl https://your-ai-subdomain.ngrok.app/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "ollama:llama3.2",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```
</CodeGroup>

## Advanced Configuration

### Restrict to Ollama Only

Block requests to cloud providers and only allow Ollama:

```yaml title="policy.yaml" highlight={4-5}
on_http_request:
  - type: ai-gateway
    config:
      only_allow_configured_providers: true
      only_allow_configured_models: true
      providers:
        - id: "ollama"
          base_url: "https://ollama.internal"
          models:
            - id: "llama3.2"
            - id: "mistral"
```

### Failover to Cloud Provider

Use Ollama as primary with automatic failover to OpenAI:

```yaml title="policy.yaml" highlight={14-17}
on_http_request:
  - type: ai-gateway
    config:
      providers:
        - id: "ollama"
          base_url: "https://ollama.internal"
          models:
            - id: "llama3.2"
        
        - id: "openai"
          api_keys:
            - value: ${secrets.get('openai', 'api-key')}
      
      model_selection:
        strategy:
          - "ai.models.filter(m, m.provider_id == 'ollama')"
          - "ai.models"
```

If Ollama is unavailable or returns an error, the gateway automatically routes to OpenAI.

### Increase Timeouts

Local models can be slower, especially on first load. Increase timeouts as needed:

```yaml title="policy.yaml" highlight={4-5}
on_http_request:
  - type: ai-gateway
    config:
      per_request_timeout: "120s"
      total_timeout: "5m"
      providers:
        - id: "ollama"
          base_url: "https://ollama.internal"
          models:
            - id: "llama3.2"
```

### Multiple Ollama Instances

Load balance across multiple machines:

```yaml title="policy.yaml" highlight={16-17}
on_http_request:
  - type: ai-gateway
    config:
      providers:
        - id: "ollama-gpu-1"
          base_url: "https://ollama-gpu-1.internal"
          models:
            - id: "llama3.2"
        
        - id: "ollama-gpu-2"
          base_url: "https://ollama-gpu-2.internal"
          models:
            - id: "llama3.2"
      
      model_selection:
        strategy:
          - "ai.models.random()"
```

### Add Model Metadata

Track model details with metadata:

```yaml title="policy.yaml" highlight={7-9,12-14}
on_http_request:
  - type: ai-gateway
    config:
      providers:
        - id: "ollama"
          base_url: "https://ollama.internal"
          metadata:
            location: "local"
            hardware: "RTX 4090"
          models:
            - id: "llama3.2"
              metadata:
                parameters: "8B"
                quantization: "Q4_K_M"
            - id: "llama3.2:70b"
              metadata:
                parameters: "70B"
                quantization: "Q4_K_M"
```

## Troubleshooting

### Connection Refused

**Symptom**: Requests fail with connection errors.

**Solutions**:
1. Verify Ollama is running: `curl http://localhost:11434/api/tags`
2. Verify ngrok tunnel is running: Check for `https://ollama.internal` in your [ngrok dashboard](https://dashboard.ngrok.com)
3. Ensure the internal endpoint URL matches your config

### Model Not Found

**Symptom**: Error saying model doesn't exist.

**Solutions**:
1. List available models: `ollama list`
2. Pull the model: `ollama pull llama3.2`
3. Verify the model ID matches exactly (including tags like `:1b`)

### Slow First Response

**Symptom**: First request takes a very long time.

**Cause**: Ollama loads models into memory on first use.

**Solutions**:
1. Increase `per_request_timeout` to allow for model loading
2. Pre-warm the model: `curl http://localhost:11434/api/generate -d '{"model":"llama3.2","prompt":""}'`
3. Keep the model loaded by sending periodic requests

### Out of Memory

**Symptom**: Ollama crashes or returns errors for large models.

**Solutions**:
1. Use a smaller model or quantized version (for example, `llama3.2:1b`)
2. Increase system RAM or use a machine with more VRAM
3. Set `OLLAMA_NUM_PARALLEL=1` to limit concurrent requests

## Next Steps

- [Custom Providers](/ai-gateway/custom-providers) - Learn about URL requirements and configuration options
- [Model Selection Strategies](/ai-gateway/guides/model-selection-strategies) - Route requests intelligently
- [Multi-Provider Failover](/ai-gateway/examples/multi-provider-failover) - Advanced failover patterns

