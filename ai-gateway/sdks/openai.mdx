---
title: OpenAI SDK
sidebarTitle: OpenAI SDK
description: Use the ngrok AI Gateway with OpenAI's official SDKs.
---

<Note>
**Prerequisite**: You need an AI Gateway endpoint before continuing. Create one using the [dashboard quickstart](/ai-gateway/quickstart) or follow the [manual setup guide](/ai-gateway/guides/creating-endpoints).
</Note>

The AI Gateway is fully compatible with OpenAI's official SDKs. Simply change the `baseURL` to route all requests through your gateway, getting automatic failover, key rotation, and observability.

## Installation

<CodeGroup>
```bash Python
pip install openai
```

```bash TypeScript
npm install openai
```

```bash Go
go get github.com/openai/openai-go/v3
```

```xml Java (Maven)
<dependency>
  <groupId>com.openai</groupId>
  <artifactId>openai-java</artifactId>
  <version>4.9.0</version>
</dependency>
```

```bash Java (Gradle)
implementation("com.openai:openai-java:4.9.0")
```

```bash .NET
dotnet add package OpenAI
```
</CodeGroup>

## Basic usage

Point the SDK at your AI Gateway endpoint.

**Which API key do I use?**
- **Passthrough mode** (default): Use your [OpenAI API key](https://platform.openai.com/api-keys) (for example, `sk-...`)
- **Server-side keys configured**: Use your [gateway auth token](/ai-gateway/guides/securing-endpoints), or omit if no auth is configured

<CodeGroup>
```python Python highlight={4}
from openai import OpenAI

client = OpenAI(
    base_url="https://your-ai-subdomain.ngrok.app/v1",
    api_key="your-api-key"
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

```typescript TypeScript highlight={4}
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "https://your-ai-subdomain.ngrok.app/v1",
  apiKey: "your-api-key",
});

const response = await client.chat.completions.create({
  model: "gpt-4o",
  messages: [{ role: "user", content: "Hello!" }],
});

console.log(response.choices[0].message.content);
```

```go Go highlight={12}
package main

import (
    "context"
    "github.com/openai/openai-go/v3"
    "github.com/openai/openai-go/v3/option"
    "github.com/openai/openai-go/v3/shared"
)

func main() {
    client := openai.NewClient(
        option.WithBaseURL("https://your-ai-subdomain.ngrok.app/v1"),
        option.WithAPIKey("your-api-key"),
    )

    chatCompletion, err := client.Chat.Completions.New(
        context.TODO(),
        openai.ChatCompletionNewParams{
            Messages: []openai.ChatCompletionMessageParamUnion{
                openai.UserMessage("Hello!"),
            },
            Model: shared.ChatModelGPT4o,
        },
    )
    if err != nil {
        panic(err.Error())
    }

    println(chatCompletion.Choices[0].Message.Content)
}
```

```java Java highlight={10}
import com.openai.client.OpenAIClient;
import com.openai.client.okhttp.OpenAIOkHttpClient;
import com.openai.models.ChatModel;
import com.openai.models.chat.completions.ChatCompletion;
import com.openai.models.chat.completions.ChatCompletionCreateParams;

public class Main {
    public static void main(String[] args) {
        OpenAIClient client = OpenAIOkHttpClient.builder()
            .baseUrl("https://your-ai-subdomain.ngrok.app/v1")
            .apiKey("your-api-key")
            .build();

        ChatCompletionCreateParams params = ChatCompletionCreateParams.builder()
            .addUserMessage("Hello!")
            .model(ChatModel.GPT_4O)
            .build();

        ChatCompletion chatCompletion = client.chat().completions().create(params);
        System.out.println(chatCompletion.choices().get(0).message().content());
    }
}
```

```csharp .NET highlight={6}
using OpenAI;
using OpenAI.Chat;

// Set the endpoint via environment variable or options
var options = new OpenAIClientOptions {
    Endpoint = new Uri("https://your-ai-subdomain.ngrok.app/v1")
};

var client = new ChatClient("gpt-4o", "your-api-key", options);

var response = await client.CompleteChatAsync("Hello!");

Console.WriteLine(response.Value.Content[0].Text);
```
</CodeGroup>

## Streaming

The AI Gateway fully supports streaming responses:

<CodeGroup>
```python Python highlight={8}
from openai import OpenAI

client = OpenAI(
    base_url="https://your-ai-subdomain.ngrok.app/v1",
    api_key="your-api-key"
)

stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Write a haiku about APIs"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

```typescript TypeScript highlight={8}
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "https://your-ai-subdomain.ngrok.app/v1",
  apiKey: "your-api-key",
});

const stream = await client.chat.completions.create({
  model: "gpt-4o",
  messages: [{ role: "user", content: "Write a haiku about APIs" }],
  stream: true,
});

for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || "");
}
```

```java Java highlight={6}
ChatCompletionCreateParams params = ChatCompletionCreateParams.builder()
    .addUserMessage("Write a haiku about APIs")
    .model(ChatModel.GPT_4O)
    .build();

client.chat().completions().createStreaming(params)
    .stream()
    .flatMap(completion -> completion.choices().stream())
    .flatMap(choice -> choice.delta().content().stream())
    .forEach(System.out::print);
```
</CodeGroup>

## Using different providers

Route to different providers using model prefixes:

<CodeGroup>
```python Python highlight={8-9}
from openai import OpenAI

client = OpenAI(
    base_url="https://your-ai-subdomain.ngrok.app/v1",
    api_key="unused"  # Gateway handles auth
)

# OpenAI
response = client.chat.completions.create(model="openai:gpt-4o", messages=[...])

# Anthropic (through the gateway)
response = client.chat.completions.create(model="anthropic:claude-3-5-sonnet-latest", messages=[...])

# Your self-hosted Ollama
response = client.chat.completions.create(model="ollama:llama3.2", messages=[...])
```

```typescript TypeScript highlight={8-9}
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "https://your-ai-subdomain.ngrok.app/v1",
  apiKey: "unused",  // Gateway handles auth
});

// OpenAI
const openaiRes = await client.chat.completions.create({ model: "openai:gpt-4o", messages: [...] });

// Anthropic (through the gateway)  
const anthropicRes = await client.chat.completions.create({ model: "anthropic:claude-3-5-sonnet-latest", messages: [...] });

// Your self-hosted Ollama
const ollamaRes = await client.chat.completions.create({ model: "ollama:llama3.2", messages: [...] });
```
</CodeGroup>

## Automatic model selection

Let the gateway choose the best model:

```python highlight={8}
from openai import OpenAI

client = OpenAI(
    base_url="https://your-ai-subdomain.ngrok.app/v1",
    api_key="unused"
)

response = client.chat.completions.create(
    model="ngrok/auto",  # Gateway selects based on your strategy
    messages=[{"role": "user", "content": "Hello!"}]
)
```

## Embeddings

Generate embeddings through the gateway:

<CodeGroup>
```python Python
response = client.embeddings.create(
    model="text-embedding-3-small",
    input="The quick brown fox jumps over the lazy dog"
)

embedding = response.data[0].embedding
```

```typescript TypeScript
const response = await client.embeddings.create({
  model: "text-embedding-3-small",
  input: "The quick brown fox jumps over the lazy dog",
});

const embedding = response.data[0].embedding;
```
</CodeGroup>

## Function calling

Tool/function calling works exactly as documented by OpenAI:

```python highlight={8-19}
from openai import OpenAI

client = OpenAI(
    base_url="https://your-ai-subdomain.ngrok.app/v1",
    api_key="your-api-key"
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What's the weather in Paris?"}],
    tools=[{
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string"}
                }
            }
        }
    }]
)
```

## Async usage

Use async clients for better performance:

<CodeGroup>
```python Python
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    base_url="https://your-ai-subdomain.ngrok.app/v1",
    api_key="your-api-key"
)

async def main():
    response = await client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": "Hello!"}]
    )
    print(response.choices[0].message.content)

asyncio.run(main())
```

```typescript TypeScript
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "https://your-ai-subdomain.ngrok.app/v1",
  apiKey: "your-api-key",
});

// TypeScript client is async by default
const response = await client.chat.completions.create({
  model: "gpt-4o",
  messages: [{ role: "user", content: "Hello!" }],
});
```

```java Java
import java.util.concurrent.CompletableFuture;

CompletableFuture<ChatCompletion> future = client.async()
    .chat()
    .completions()
    .create(params);

future.thenAccept(completion -> {
    System.out.println(completion.choices().get(0).message().content());
});
```
</CodeGroup>

## Error handling

The gateway handles many errors automatically through failover. For errors that reach your app:

```python
from openai import OpenAI, APIError, RateLimitError

client = OpenAI(
    base_url="https://your-ai-subdomain.ngrok.app/v1",
    api_key="your-api-key"
)

try:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": "Hello!"}]
    )
except RateLimitError:
    # All configured keys exhausted
    print("Rate limited across all providers")
except APIError as e:
    print(f"API error: {e}")
```

## Supported endpoints

The AI Gateway supports these OpenAI API endpoints:

| Endpoint | Description |
|----------|-------------|
| `/v1/chat/completions` | Chat completions (GPT-4, Claude, etc.) |
| `/v1/completions` | Legacy completions |
| `/v1/embeddings` | Text embeddings |
| `/v1/models` | List available models |

## Next steps

- [Model Selection Strategies](/ai-gateway/guides/model-selection-strategies) - Configure routing logic
- [Configuring Providers](/ai-gateway/guides/configuring-providers) - Set up providers and keys
- [Multi-Provider Failover](/ai-gateway/examples/multi-provider-failover) - Failover examples
