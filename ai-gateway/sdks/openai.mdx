---
title: OpenAI SDK
sidebarTitle: OpenAI SDK
description: Use the ngrok AI Gateway with OpenAI's official SDKs.
---


<Note>
  **Prerequisite**: You need an AI Gateway endpoint before continuing.
  Create one using the [dashboard quickstart](/ai-gateway/quickstart) or follow the [manual setup guide](/ai-gateway/guides/creating-endpoints).
</Note>

The AI Gateway is fully compatible with OpenAI's official SDKs.
Simply change the base URL configuration option for the OpenAI SDK to route all requests through your gateway, get automatic failover, key rotation, and observability.

## Installation

<CodeGroup>
  ```bash Python
  pip install openai
  ```

  ```bash TypeScript
  npm install openai
  ```
</CodeGroup>

## Basic usage

Point the SDK at your AI Gateway endpoint.

**Which API key do I use?**

- **Passthrough mode** (default): Use your [OpenAI API key](https://platform.openai.com/api-keys) (for example, `sk-...`)
- **Server-side keys configured**: Use your [gateway auth token](/ai-gateway/guides/securing-endpoints), or omit if no auth is configured

<CodeGroup>
  ```python Python highlight={4}
  from openai import OpenAI

  client = OpenAI(
  base_url="https://your-ai-gateway.ngrok.app/v1",
  api_key="your-api-key"
  )

  response = client.chat.completions.create(
  model="gpt-4o",
  messages=[{"role": "user", "content": "Hello!"}]
  )

  print(response.choices[0].message.content)

  ````

  ```typescript TypeScript highlight={4}
  import OpenAI from "openai";

  const client = new OpenAI({
  baseURL: "https://your-ai-gateway.ngrok.app/v1",
  apiKey: "your-api-key",
  });

  const response = await client.chat.completions.create({
  model: "gpt-4o",
  messages: [{ role: "user", content: "Hello!" }],
  });

  console.log(response.choices[0].message.content);
  ````
</CodeGroup>

## Streaming

The AI Gateway fully supports streaming responses:

<CodeGroup>
  ```python Python highlight={8}
  from openai import OpenAI

  client = OpenAI(
  base_url="https://your-ai-gateway.ngrok.app/v1",
  api_key="your-api-key"
  )

  stream = client.chat.completions.create(
  model="gpt-4o",
  messages=[{"role": "user", "content": "Write a haiku about APIs"}],
  stream=True
  )

  for chunk in stream:
  if chunk.choices[0].delta.content:
  print(chunk.choices[0].delta.content, end="")

  ````

  ```typescript TypeScript highlight={8}
  import OpenAI from "openai";

  const client = new OpenAI({
  baseURL: "https://your-ai-gateway.ngrok.app/v1",
  apiKey: "your-api-key",
  });

  const stream = await client.chat.completions.create({
  model: "gpt-4o",
  messages: [{ role: "user", content: "Write a haiku about APIs" }],
  stream: true,
  });

  for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || "");
  }
  ````
</CodeGroup>

## Using different providers

Route to different providers using model prefixes:

<CodeGroup>
  ```python Python highlight={8-9}
  from openai import OpenAI

  client = OpenAI(
  base_url="https://your-ai-gateway.ngrok.app/v1",
  api_key="unused" # Gateway handles auth
  )

  # OpenAI

  response = client.chat.completions.create(model="openai:gpt-4o", messages=[...])

  # Anthropic (through the gateway)

  response = client.chat.completions.create(model="anthropic:claude-3-5-sonnet-latest", messages=[...])

  # Your self-hosted Ollama

  response = client.chat.completions.create(model="ollama:llama3.2", messages=[...])

  ````

  ```typescript TypeScript highlight={8-9}
  import OpenAI from "openai";

  const client = new OpenAI({
  baseURL: "https://your-ai-gateway.ngrok.app/v1",
  apiKey: "unused",  // Gateway handles auth
  });

  // OpenAI
  const openaiRes = await client.chat.completions.create({ model: "openai:gpt-4o", messages: [...] });

  // Anthropic (through the gateway)
  const anthropicRes = await client.chat.completions.create({ model: "anthropic:claude-3-5-sonnet-latest", messages: [...] });

  // Your self-hosted Ollama
  const ollamaRes = await client.chat.completions.create({ model: "ollama:llama3.2", messages: [...] });
  ````
</CodeGroup>

## Automatic model selection

Let the gateway choose the best model:

<CodeGroup>
  ```python Python highlight={9}
  from openai import OpenAI

  client = OpenAI(
  base_url="https://your-ai-gateway.ngrok.app/v1",
  api_key="unused"
  )

  response = client.chat.completions.create(
  model="ngrok/auto", # Gateway selects based on your strategy
  messages=[{"role": "user", "content": "Hello!"}]
  )

  ````

  ```typescript TypeScript highlight={9}
  import OpenAI from "openai";

  const client = new OpenAI({
  baseURL: "https://your-ai-gateway.ngrok.app/v1",
  apiKey: "your-api-key",
  });

  const response = await client.chat.completions.create({
  model: "ngrok/auto", // Gateway selects based on your strategy
  messages: [{ role: "user", content: "Hello!" }],
  });
  ````
</CodeGroup>

## Embeddings

Generate embeddings through the gateway:

<CodeGroup>
  ```python Python
  response = client.embeddings.create(
    model="openai:text-embedding-3-small",
    input="The quick brown fox jumps over the lazy dog"
  )

  embedding = response.data[0].embedding

  ````

  ```typescript TypeScript
  const response = await client.embeddings.create({
  model: "openai:text-embedding-3-small",
  input: "The quick brown fox jumps over the lazy dog",
  });

  const embedding = response.data[0].embedding;
  ````
</CodeGroup>

## Function calling

Tool/function calling works exactly as documented by OpenAI:

```python highlight={11-23}
from openai import OpenAI

client = OpenAI(
    base_url="https://your-ai-gateway.ngrok.app/v1",
    api_key="your-api-key"
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What's the weather in Paris?"}],
    tools=[{
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string"}
                }
            }
        }
    }]
)
```

## Async usage

Use async clients for better performance:

<CodeGroup>
  ```python Python
  import asyncio
  from openai import AsyncOpenAI

  client = AsyncOpenAI(
  base_url="https://your-ai-gateway.ngrok.app/v1",
  api_key="your-api-key"
  )

  async def main():
  response = await client.chat.completions.create(
  model="gpt-4o",
  messages=[{"role": "user", "content": "Hello!"}]
  )
  print(response.choices[0].message.content)

  asyncio.run(main())

  ````

  ```typescript TypeScript
  import OpenAI from "openai";

  const client = new OpenAI({
  baseURL: "https://your-ai-gateway.ngrok.app/v1",
  apiKey: "your-api-key",
  });

  // TypeScript client is async by default
  const response = await client.chat.completions.create({
  model: "gpt-4o",
  messages: [{ role: "user", content: "Hello!" }],
  });
  ````
</CodeGroup>

## Error handling

The gateway handles many errors automatically through failover.
For errors that reach your app:

<CodeGroup>
  ```python Python
  from openai import OpenAI, APIError, RateLimitError

  client = OpenAI(
  base_url="https://your-ai-gateway.ngrok.app/v1",
  api_key="your-api-key"
  )

  try:
  response = client.chat.completions.create(
  model="gpt-4o",
  messages=[{"role": "user", "content": "Hello!"}]
  )
  except RateLimitError: # All configured keys exhausted
  print("Rate limited across all providers")
  except APIError as e:
  print(f"API error: {e}")

  ````

  ```typescript TypeScript
  import OpenAI, { APIError, RateLimitError } from "openai";

  const client = new OpenAI({
  baseURL: "https://your-ai-gateway.ngrok.app/v1",
  apiKey: "your-api-key",
  });

  try {
  const response = await client.chat.completions.create({
    model: "gpt-4o",
    messages: [{ role: "user", content: "Hello!" }],
  });
  } catch (e) {
  if (e instanceof RateLimitError) {
    // All configured keys exhausted
    console.log("Rate limited across all providers");
  } else if (e instanceof APIError) {
    console.log(`API error: ${e.message}`);
  }
  }
  ````
</CodeGroup>

## Supported endpoints

The AI Gateway supports these OpenAI API endpoints:

| Endpoint | Description | | ---------------------- | ------------------ | | `/v1/chat/completions` | Chat completions | | `/v1/completions` | Legacy completions | | `/v1/embeddings` | Text embeddings | | `/v1/responses` | Responses |

## Next steps

- [Model Selection Strategies](/ai-gateway/guides/model-selection-strategies) - Configure routing logic
- [Configuring Providers](/ai-gateway/guides/configuring-providers) - Set up providers and keys
- [Multi-Provider Failover](/ai-gateway/examples/multi-provider-failover) - Failover examples
